{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "corporate-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import seaborn as sn\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.utils.extmath import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import itertools\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "checked-rugby",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2onehot(y, list_classes):\n",
    "    \"\"\"\n",
    "    y = list of class lables of length n\n",
    "    output = n x k array, i th row = one-hot encoding of y[i] (e.g., [0,0,1,0,0])\n",
    "    \"\"\"\n",
    "    Y = np.zeros(shape = [len(y), len(list_classes)], dtype=int)\n",
    "    for i in np.arange(Y.shape[0]):\n",
    "        for j in np.arange(len(list_classes)):\n",
    "            if y[i] == list_classes[j]:\n",
    "                Y[i,j] = 1\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "final-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot2list(y, list_classes=None):\n",
    "    \"\"\"\n",
    "    y = n x k array, i th row = one-hot encoding of y[i] (e.g., [0,0,1,0,0])\n",
    "    output =  list of class lables of length n\n",
    "    \"\"\"\n",
    "    if list_classes is None:\n",
    "        list_classes = np.arange(y.shape[1])\n",
    "        \n",
    "    y_list = []\n",
    "    for i in np.arange(y.shape[0]):\n",
    "        idx = np.where(y[i,:]==1)\n",
    "        idx = idx[0][0]\n",
    "        y_list.append(list_classes[idx])\n",
    "    return y_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-syntax",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "isolated-telling",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  sex  cp  trtbps  chol  fbs  restecg  thalachh  exng  oldpeak  slp  \\\n",
      "0   63    1   3     145   233    1        0       150     0      2.3    0   \n",
      "1   37    1   2     130   250    0        1       187     0      3.5    0   \n",
      "2   41    0   1     130   204    0        0       172     0      1.4    2   \n",
      "3   56    1   1     120   236    0        1       178     0      0.8    2   \n",
      "4   57    0   0     120   354    0        1       163     1      0.6    2   \n",
      "\n",
      "   caa  thall  output  \n",
      "0    0      1       1  \n",
      "1    0      2       1  \n",
      "2    0      2       1  \n",
      "3    0      2       1  \n",
      "4    0      2       1  \n"
     ]
    }
   ],
   "source": [
    "# read csv data\n",
    "heart = pd.read_csv(\"heart.csv\")\n",
    "print(heart.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "frequent-sector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 13)\n",
      "(303,)\n"
     ]
    }
   ],
   "source": [
    "# separate X and y\n",
    "X = np.array(heart.iloc[:,:-1])\n",
    "print(X.shape)\n",
    "y = np.array(heart.iloc[:,13])\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tropical-basket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 27)\n"
     ]
    }
   ],
   "source": [
    "# prevent overflow warning\n",
    "X[:,np.r_[0,3,4,7]] = X[:,np.r_[0,3,4,7]]/100\n",
    "\n",
    "# onehot2list categorical variables\n",
    "X = np.delete(X, [2,6,10,11,12], 1)\n",
    "#X = np.delete(X, [11], 1)\n",
    "rcg = list2onehot(heart.iloc[:,6].tolist(), [0,1,2])\n",
    "cp = list2onehot(heart.iloc[:,2].tolist(), [0,1,2,3])\n",
    "slp = list2onehot(heart.iloc[:,10].tolist(), [0,1,2])\n",
    "caa = list2onehot(heart.iloc[:,11].tolist(), [0,1,2,3,4])\n",
    "thall = list2onehot(heart.iloc[:,12].tolist(), [0,1,2,3])\n",
    "\n",
    "X = np.hstack((X, rcg, cp, slp, caa, thall))\n",
    "#X = np.hstack((X, cp, slp, thall))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "binary-village",
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = list2onehot(y.tolist(), [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "controlling-haiti",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (233, 27)\n",
      "X_test.shape (70, 27)\n",
      "y_train.shape (233, 2)\n",
      "y_test.shape (70, 2)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into train and test sets\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "y_train = []\n",
    "\n",
    "for i in np.arange(X.shape[0]):\n",
    "    # for each example i, make it into train set with probabiliy 0.8 and into test set otherwise \n",
    "    U = np.random.rand() # Uniform([0,1]) variable\n",
    "    if U<0.8:\n",
    "        X_train.append(X[i,:])\n",
    "        y_train.append(y0[i])\n",
    "    else:\n",
    "        X_test.append(X[i,:])\n",
    "        y_test.append(y0[i])\n",
    "X_train = np.asarray(X_train)\n",
    "X_test = np.asarray(X_test)\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "print('X_train.shape', X_train.shape)\n",
    "print('X_test.shape', X_test.shape)\n",
    "print('y_train.shape', y_train.shape)\n",
    "print('y_test.shape', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "marine-clothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid and logit function \n",
    "def sigmoid(x):\n",
    "    return np.exp(x)/(1+np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "second-parts",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_MLR_GD(Y, H, W0=None, sub_iter=100, stopping_diff=0.01):\n",
    "        '''\n",
    "        Convex optimization algorithm for Multiclass Logistic Regression using Gradient Descent \n",
    "        Y = (n x k), H = (p x n) (\\Phi in lecture note), W = (p x k)\n",
    "        Multiclass Logistic Regression: Y ~ vector of discrete RVs with PMF = sigmoid(H.T @ W)\n",
    "        MLE -->\n",
    "        Find \\hat{W} = argmin_W ( sum_j ( log(1+exp(H_j.T @ W) ) - Y.T @ H.T @ W ) )\n",
    "        '''\n",
    "        k = Y.shape[1] # number of classes \n",
    "        if W0 is None:\n",
    "            W0 = 2*np.random.rand(H.shape[0],k)-1 #If initial coefficients W0 is None, randomly initialize  \n",
    "            \n",
    "        W1 = W0.copy()\n",
    "        i = 0\n",
    "        grad = np.ones(W0.shape)\n",
    "        while (i < sub_iter) and (np.linalg.norm(grad) > stopping_diff):\n",
    "            Q = 1/(1+np.exp(-H.T @ W1))  # probability matrix, same shape as Y\n",
    "            # grad = H @ (Q - Y).T + alpha * np.ones(W0.shape[1])\n",
    "            grad = H @ (Q - Y)\n",
    "            W1 = W1 - (np.log(i+1) / (((i + 1) ** (0.5)))) * grad\n",
    "            i = i + 1\n",
    "            # print('iter %i, grad_norm %f' %(i, np.linalg.norm(grad)))\n",
    "        return W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "married-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_accuracy_metrics(Y_test, P_pred, class_labels=None, use_opt_threshold=False):\n",
    "    # y_test = multiclass one-hot encoding  labels \n",
    "    # Q = predicted probability for y_test\n",
    "    # compuate various classification accuracy metrics\n",
    "    results_dict = {}\n",
    "    y_test = []\n",
    "    y_pred = []\n",
    "    for i in np.arange(Y_test.shape[0]):\n",
    "        for j in np.arange(Y_test.shape[1]):\n",
    "            if Y_test[i,j] == 1:\n",
    "                y_test.append(j)\n",
    "            if P_pred[i,j] == np.max(P_pred[i,:]):\n",
    "                # print('!!!', np.where(P_pred[i,:]==np.max(P_pred[i,:])))\n",
    "                y_pred.append(j)\n",
    "            \n",
    "    confusion_mx = metrics.confusion_matrix(y_test, y_pred)\n",
    "    print('!!! confusion_mx', confusion_mx)\n",
    "    results_dict.update({'confusion_mx':confusion_mx})\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "burning-steel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! confusion_mx [[32  2]\n",
      " [18 18]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-263749036d71>:17: RuntimeWarning: overflow encountered in exp\n",
      "  Q = 1/(1+np.exp(-H.T @ W1))  # probability matrix, same shape as Y\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the multiclass logistic regression model \n",
    "H_train = np.vstack((np.ones(X_train.shape[0]), X_train.T))  # add first row of 1's for bias features \n",
    "W = fit_MLR_GD(Y=y_train, H=H_train) \n",
    "\n",
    "# Get predicted probabilities \n",
    "H_test = np.vstack((np.ones(X_test.shape[0]), X_test.T))\n",
    "Q = softmax(H_test.T @ W.copy()) # predicted probabilities for y_test # Uses sklearn's softmax for numerical stability\n",
    "\n",
    "results_dict = multiclass_accuracy_metrics(Y_test=y_test, P_pred=Q)\n",
    "confusion_mx = results_dict.get('confusion_mx')\n",
    "\n",
    "sn.heatmap(confusion_mx, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "irish-barrel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_LR_GD(Y, H, W0=None, sub_iter=100, stopping_diff=0.01):\n",
    "        '''\n",
    "        Convex optimization algorithm for Logistic Regression using Gradient Descent \n",
    "        Y = (n x 1), H = (p x n) (\\Phi in lecture note), W = (p x 1)\n",
    "        Logistic Regression: Y ~ Bernoulli(Q), Q = sigmoid(H.T @ W)\n",
    "        MLE -->\n",
    "        Find \\hat{W} = argmin_W ( sum_j ( log(1+exp(H_j.T @ W) ) - Y.T @ H.T @ W ) )\n",
    "        '''\n",
    "        if W0 is None:\n",
    "            W0 = np.random.rand(H.shape[0],1) #If initial coefficients W0 is None, randomly initialize  \n",
    "            \n",
    "        \n",
    "        \n",
    "        W1 = W0.copy()\n",
    "        i = 0\n",
    "        grad = np.ones(W0.shape)\n",
    "        while (i < sub_iter) and (np.linalg.norm(grad) > stopping_diff):\n",
    "            Q = 1/(1+np.exp(-H.T @ W1))  # probability matrix, same shape as Y\n",
    "            # grad = H @ (Q - Y).T + alpha * np.ones(W0.shape[1])\n",
    "            grad = H @ (Q - Y)\n",
    "            W1 = W1 - (np.log(i+1) / (((i + 1) ** (0.5)))) * grad\n",
    "            i = i + 1\n",
    "            # print('iter %i, grad_norm %f' %(i, np.linalg.norm(grad)))\n",
    "        return W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "identical-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_metrics(Y_test, P_pred, use_opt_threshold=False):\n",
    "    # y_test = binary label \n",
    "    # Q = predicted probability for y_test\n",
    "    # compuate various binary classification accuracy metrics\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(Y_test, P_pred, pos_label=None)\n",
    "    mythre = thresholds[np.argmax(tpr - fpr)]\n",
    "    myauc = metrics.auc(fpr, tpr)\n",
    "    # print('!!! auc', myauc)\n",
    "    \n",
    "    # Compute classification statistics\n",
    "    threshold = 0.5\n",
    "    if use_opt_threshold:\n",
    "        threshold = mythre\n",
    "    \n",
    "    Y_pred = Q.copy()\n",
    "    Y_pred[Y_pred < threshold] = 0\n",
    "    Y_pred[Y_pred >= threshold] = 1\n",
    "\n",
    "    mcm = confusion_matrix(Y_test, Y_pred)\n",
    "    tn = mcm[0, 0]\n",
    "    tp = mcm[1, 1]\n",
    "    fn = mcm[1, 0]\n",
    "    fp = mcm[0, 1]\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    sensitivity = tn / (tn + fp)\n",
    "    specificity = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    fall_out = fp / (fp + tn)\n",
    "    miss_rate = fn / (fn + tp)\n",
    "\n",
    "    # Save results\n",
    "    results_dict = {}\n",
    "    results_dict.update({'Y_test': Y_test})\n",
    "    results_dict.update({'Y_pred': Y_pred})\n",
    "    results_dict.update({'AUC': myauc})\n",
    "    results_dict.update({'Opt_threshold': mythre})\n",
    "    results_dict.update({'Accuracy': accuracy})\n",
    "    results_dict.update({'Sensitivity': sensitivity})\n",
    "    results_dict.update({'Specificity': specificity})\n",
    "    results_dict.update({'Precision': precision})\n",
    "    results_dict.update({'Fall_out': fall_out})\n",
    "    results_dict.update({'Miss_rate': miss_rate})\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "recognized-sullivan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (237, 27)\n",
      "X_test.shape (66, 27)\n",
      "y_train.shape (237, 1)\n",
      "y_test.shape (66, 1)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into train and test sets\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "y_train = []\n",
    "\n",
    "for i in np.arange(X.shape[0]):\n",
    "    # for each example i, make it into train set with probabiliy 0.8 and into test set otherwise \n",
    "    U = np.random.rand() # Uniform([0,1]) variable\n",
    "    if U<0.8:\n",
    "        X_train.append(X[i,:])\n",
    "        y_train.append(y[i])\n",
    "    else:\n",
    "        X_test.append(X[i,:])\n",
    "        y_test.append(y[i])\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "X_test = np.asarray(X_test)\n",
    "y_train = np.asarray(y_train).reshape(-1,1)\n",
    "y_test = np.asarray(y_test).reshape(-1,1)\n",
    "\n",
    "print('X_train.shape', X_train.shape)\n",
    "print('X_test.shape', X_test.shape)\n",
    "print('y_train.shape', y_train.shape)\n",
    "print('y_test.shape', y_test.shape)\n",
    "#print('y_test', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "graduate-faith",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.905331\n",
      "Opt_threshold = 0.000000\n",
      "Accuracy = 0.818182\n",
      "Sensitivity = 0.937500\n",
      "Specificity = 0.705882\n",
      "Precision = 0.923077\n",
      "Fall_out = 0.062500\n",
      "Miss_rate = 0.294118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-7ee600204434>:18: RuntimeWarning: overflow encountered in exp\n",
      "  Q = 1/(1+np.exp(-H.T @ W1))  # probability matrix, same shape as Y\n"
     ]
    }
   ],
   "source": [
    "# fit logistic regression using GD and compute binary classification accuracies\n",
    "\n",
    "# Train the logistic regression model \n",
    "H_train = np.vstack((np.ones(X_train.shape[0]), X_train.T))  # add first row of 1's for bias features \n",
    "W = fit_LR_GD(Y=y_train, H=H_train)\n",
    "\n",
    "# Get predicted probabilities \n",
    "H_test = np.vstack((np.ones(X_test.shape[0]), X_test.T))\n",
    "Q = 1 / (1 + np.exp(-H_test.T @ W)) # predicted probabilities for y_test\n",
    "\n",
    "# Compute binary classification accuracies\n",
    "results_dict = compute_accuracy_metrics(Y_test=y_test, P_pred = Q)\n",
    "\n",
    "# Print out the results \n",
    "\n",
    "keys_list = [i for i in results_dict.keys()]\n",
    "for key in keys_list:\n",
    "    if key not in ['Y_test', 'Y_pred']:\n",
    "        print('%s = %f' % (key, results_dict.get(key)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-instrumentation",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "artistic-coating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  sex  cp  trtbps  chol  fbs  restecg  thalachh  exng  oldpeak  slp  \\\n",
      "0   63    1   3     145   233    1        0       150     0      2.3    0   \n",
      "1   37    1   2     130   250    0        1       187     0      3.5    0   \n",
      "2   41    0   1     130   204    0        0       172     0      1.4    2   \n",
      "3   56    1   1     120   236    0        1       178     0      0.8    2   \n",
      "4   57    0   0     120   354    0        1       163     1      0.6    2   \n",
      "\n",
      "   caa  thall  output  \n",
      "0    0      1       1  \n",
      "1    0      2       1  \n",
      "2    0      2       1  \n",
      "3    0      2       1  \n",
      "4    0      2       1  \n"
     ]
    }
   ],
   "source": [
    "heart = pd.read_csv(\"heart.csv\")\n",
    "print(heart.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "instrumental-lodge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    age  sex  cp  trtbps  chol  fbs  restecg  thalachh  exng  oldpeak  slp  \\\n",
      "0  0.63    1   3    1.45  2.33    1        0      1.50     0      2.3    0   \n",
      "1  0.37    1   2    1.30  2.50    0        1      1.87     0      3.5    0   \n",
      "2  0.41    0   1    1.30  2.04    0        0      1.72     0      1.4    2   \n",
      "3  0.56    1   1    1.20  2.36    0        1      1.78     0      0.8    2   \n",
      "4  0.57    0   0    1.20  3.54    0        1      1.63     1      0.6    2   \n",
      "\n",
      "   caa  thall  output  \n",
      "0    0      1       1  \n",
      "1    0      2       1  \n",
      "2    0      2       1  \n",
      "3    0      2       1  \n",
      "4    0      2       1  \n"
     ]
    }
   ],
   "source": [
    "#preventing overflow errors\n",
    "large_cols = [\"age\", \"trtbps\", \"chol\", \"thalachh\"]\n",
    "for cols in large_cols:\n",
    "    heart[cols]/= 100\n",
    "print(heart.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "british-lithuania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 13)\n",
      "(303,)\n"
     ]
    }
   ],
   "source": [
    "# separate X and y\n",
    "X = heart.drop(\"output\", axis =1)\n",
    "print(X.shape)\n",
    "y = np.array(heart.iloc[:,13])\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "roman-water",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_vectors(df, classifiers):\n",
    "    feature_vect_list = []\n",
    "    for i in range(df.shape[0]):\n",
    "        feature_vect = []\n",
    "        for col in df.columns:\n",
    "            value = df.loc[i][col]\n",
    "            if col not in classifiers.keys():\n",
    "                feature_vect.append(value)\n",
    "            else:\n",
    "                for possible_value in classifiers[col]:\n",
    "                    if value == possible_value:\n",
    "                        feature_vect.append(1)\n",
    "                    else:\n",
    "                        feature_vect.append(0)\n",
    "        #print(len(feature_vect))\n",
    "        feature_vect_list.append(feature_vect)\n",
    "    feature_vect_list = np.asarray(feature_vect_list)\n",
    "    return feature_vect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bright-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2onehot(y, list_classes):\n",
    "    \"\"\"\n",
    "    y = list of class lables of length n\n",
    "    output = n x k array, i th row = one-hot encoding of y[i] (e.g., [0,0,1,0,0])\n",
    "    \"\"\"\n",
    "    Y = np.zeros(shape = [len(y), len(list_classes)], dtype=int)\n",
    "    for i in np.arange(Y.shape[0]):\n",
    "        for j in np.arange(len(list_classes)):\n",
    "            if y[i] == list_classes[j]:\n",
    "                Y[i,j] = 1\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "narrow-jacksonville",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define classifiers\n",
    "classifiers = {\"cp\": [], \"slp\": [], \"thall\": [], \"restecg\": [], \"caa\": []}\n",
    "#assigns classifiers with all the possible classifications\n",
    "for col in heart.columns:\n",
    "    if col in classifiers.keys():\n",
    "        for value in heart[col]:\n",
    "            if value not in classifiers[col]:\n",
    "                classifiers[col].append(value)\n",
    "        classifiers[col].sort()\n",
    "\n",
    "X = create_feature_vectors(X, classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "desirable-index",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 2)\n"
     ]
    }
   ],
   "source": [
    "y0 = list2onehot(y.tolist(), [0,1])\n",
    "print(y0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "pleasant-affiliate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_MNB(Y, H):\n",
    "    '''\n",
    "    Fit Multinomial Naive Bayes Calssifier\n",
    "    Use the Maximum Likelihood prior and class conditional probabilities (in closed forms)\n",
    "    Y = (n x k), H = (p x n) (\\Phi in lecture note), W = (p x k)\n",
    "    prior on class labels = empirical PMF = [ # class i examples / total ]\n",
    "    class-conditional for class i = [ # word j in class i examples / # words in class i examples]\n",
    "    Output = prior (k, ), class_conditional_PMF = (k, p)\n",
    "    '''\n",
    "    k = Y.shape[1] # number of classes \n",
    "\n",
    "    prior = np.sum(Y, axis=0)/np.sum(np.sum(Y, axis=0))\n",
    "\n",
    "    class_conditional_PMF = []\n",
    "    for i in np.arange(Y.shape[1]):\n",
    "        idx = np.where(Y[:,i]==1)\n",
    "        sub_H = H[:,idx[0]] + 1# add psuedocount\n",
    "        word_count_per_class = np.sum(sub_H, axis=1)\n",
    "        class_conditional_PMF.append(word_count_per_class/np.sum(word_count_per_class))\n",
    "\n",
    "    return prior, np.asarray(class_conditional_PMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "concrete-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_MNB(X_test, prior, class_conditional_PMF):\n",
    "    '''\n",
    "    Compute predicted PMF for the test data given prior and class_conditional_PMF\n",
    "    Simple use of Bayes' Theorem \n",
    "    X_test = (p x n) (words x docs)\n",
    "    '''\n",
    "    print(X_test.shape)\n",
    "    print(class_conditional_PMF.shape)\n",
    "    \n",
    "    P = class_conditional_PMF / np.min(class_conditional_PMF) # normalize so that log(P) is not too small \n",
    "    Q = np.exp(X_test @ np.log(P).T)\n",
    "    Q = Q * np.repeat(prior[:, np.newaxis], repeats=Q.shape[0], axis=1).T\n",
    "    sum_of_rows = Q.sum(axis=1)\n",
    "    \n",
    "    \n",
    "    return Q / sum_of_rows[:, np.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fundamental-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_accuracy_metrics(Y_test, P_pred, class_labels=None, use_opt_threshold=False):\n",
    "    # y_test = multiclass one-hot encoding  labels \n",
    "    # Q = predicted probability for y_test\n",
    "    # compuate various classification accuracy metrics\n",
    "    results_dict = {}\n",
    "    y_test = []\n",
    "    y_pred = []\n",
    "    for i in np.arange(Y_test.shape[0]):\n",
    "        for j in np.arange(Y_test.shape[1]):\n",
    "            if Y_test[i,j] == 1:\n",
    "                y_test.append(j)\n",
    "            if P_pred[i,j] == np.max(P_pred[i,:]):\n",
    "                # print('!!!', np.where(P_pred[i,:]==np.max(P_pred[i,:])))\n",
    "                y_pred.append(j)\n",
    "                \n",
    "    print(\"y_test\", len(y_test))\n",
    "    print(\"y_pred\", len(y_pred))\n",
    "    \n",
    "    confusion_mx = metrics.confusion_matrix(y_test, y_pred)\n",
    "    print('!!! confusion_mx', confusion_mx)\n",
    "    results_dict.update({'confusion_mx':confusion_mx})\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "static-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot2list(y, list_classes=None):\n",
    "    \"\"\"\n",
    "    y = n x k array, i th row = one-hot encoding of y[i] (e.g., [0,0,1,0,0])\n",
    "    output =  list of class lables of length n\n",
    "    \"\"\"\n",
    "    if list_classes is None:\n",
    "        list_classes = np.arange(y.shape[1])\n",
    "        \n",
    "    y_list = []\n",
    "    for i in np.arange(y.shape[0]):\n",
    "        idx = np.where(y[i,:]==1)\n",
    "        idx = idx[0][0]\n",
    "        y_list.append(list_classes[idx])\n",
    "    return y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "completed-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_metrics(Y_test, P_pred, use_opt_threshold=False):\n",
    "    # y_test = binary label \n",
    "    # Q = predicted probability for y_test\n",
    "    # compuate various binary classification accuracy metrics\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(Y_test, P_pred, pos_label=None)\n",
    "    mythre = thresholds[np.argmax(tpr - fpr)]\n",
    "    myauc = metrics.auc(fpr, tpr)\n",
    "    # print('!!! auc', myauc)\n",
    "    \n",
    "    # Compute classification statistics\n",
    "    threshold = 0.5\n",
    "    if use_opt_threshold:\n",
    "        threshold = mythre\n",
    "    \n",
    "    Y_pred = P_pred.copy()\n",
    "    Y_pred[Y_pred < threshold] = 0\n",
    "    Y_pred[Y_pred >= threshold] = 1\n",
    "\n",
    "    mcm = confusion_matrix(Y_test, Y_pred)\n",
    "    tn = mcm[0, 0]\n",
    "    tp = mcm[1, 1]\n",
    "    fn = mcm[1, 0]\n",
    "    fp = mcm[0, 1]\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    sensitivity = tn / (tn + fp)\n",
    "    specificity = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    fall_out = fp / (fp + tn)\n",
    "    miss_rate = fn / (fn + tp)\n",
    "\n",
    "    # Save results\n",
    "    results_dict = {}\n",
    "    results_dict.update({'Y_test': Y_test})\n",
    "    results_dict.update({'Y_pred': Y_pred})\n",
    "    results_dict.update({'AUC': myauc})\n",
    "    results_dict.update({'Opt_threshold': mythre})\n",
    "    results_dict.update({'Accuracy': accuracy})\n",
    "    results_dict.update({'Sensitivity': sensitivity})\n",
    "    results_dict.update({'Specificity': specificity})\n",
    "    results_dict.update({'Precision': precision})\n",
    "    results_dict.update({'Fall_out': fall_out})\n",
    "    results_dict.update({'Miss_rate': miss_rate})\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "interpreted-destination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (235, 27)\n",
      "X_test.shape (68, 27)\n",
      "y_train.shape (235, 2)\n",
      "y_test.shape (68, 2)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into train and test sets\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "y_train = []\n",
    "\n",
    "for i in np.arange(X.shape[0]):\n",
    "    # for each example i, make it into train set with probabiliy 0.8 and into test set otherwise \n",
    "    U = np.random.rand() # Uniform([0,1]) variable\n",
    "    if U<0.8:\n",
    "        X_train.append(X[i,:])\n",
    "        y_train.append(y0[i])\n",
    "    else:\n",
    "        X_test.append(X[i,:])\n",
    "        y_test.append(y0[i])\n",
    "X_train = np.asarray(X_train)\n",
    "X_test = np.asarray(X_test)\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "print('X_train.shape', X_train.shape)\n",
    "print('X_test.shape', X_test.shape)\n",
    "print('y_train.shape', y_train.shape)\n",
    "print('y_test.shape', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "charged-rebecca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68, 27)\n",
      "(2, 27)\n",
      "y_test 68\n",
      "y_pred 68\n",
      "!!! confusion_mx [[32  8]\n",
      " [ 2 26]]\n",
      "\n",
      "AUC = 0.919643\n",
      "Opt_threshold = 0.549380\n",
      "Accuracy = 0.852941\n",
      "Sensitivity = 0.800000\n",
      "Specificity = 0.928571\n",
      "Precision = 0.764706\n",
      "Fall_out = 0.200000\n",
      "Miss_rate = 0.071429\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prior, class_cond_PMF = fit_MNB(Y = y_train, H=X_train.T)\n",
    "\n",
    "predict_test = predict_MNB(X_test, prior, class_cond_PMF)\n",
    "\n",
    "\n",
    "# Compute binary classification accuracies\n",
    "results_dict1 = multiclass_accuracy_metrics(Y_test=y_test, P_pred = predict_test)\n",
    "\n",
    "\n",
    "y_test2 = np.asarray(onehot2list(y_test))\n",
    "results_dict2 = compute_accuracy_metrics(Y_test = y_test2, P_pred = predict_test[:,1])\n",
    "\n",
    "\n",
    "# Print out the results \n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=[4,4])\n",
    "\n",
    "confusion_mx = results_dict1.get(\"confusion_mx\")\n",
    "im =ax.imshow(confusion_mx)\n",
    "\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.01, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "\n",
    "print()\n",
    "\n",
    "keys_list = [i for i in results_dict2.keys()]\n",
    "for key in keys_list:\n",
    "    if key not in ['Y_test', 'Y_pred']:\n",
    "        print('%s = %f' % (key, results_dict2.get(key)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-thong",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cutting-workplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2onehot(y, list_classes):\n",
    "    \"\"\"\n",
    "    y = list of class lables of length n\n",
    "    output = n x k array, i th row = one-hot encoding of y[i] (e.g., [0,0,1,0,0])\n",
    "    \"\"\"\n",
    "    Y = np.zeros(shape = [len(y), len(list_classes)], dtype=int)\n",
    "    for i in np.arange(Y.shape[0]):\n",
    "        for j in np.arange(len(list_classes)):\n",
    "            if y[i] == list_classes[j]:\n",
    "                Y[i,j] = 1\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "analyzed-smoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot2list(y, list_classes=None):\n",
    "    \"\"\"\n",
    "    y = n x k array, i th row = one-hot encoding of y[i] (e.g., [0,0,1,0,0])\n",
    "    output =  list of class lables of length n\n",
    "    \"\"\"\n",
    "    if list_classes is None:\n",
    "        list_classes = np.arange(y.shape[1])\n",
    "        \n",
    "    y_list = []\n",
    "    for i in np.arange(y.shape[0]):\n",
    "        idx = np.where(y[i,:]==1)\n",
    "        idx = idx[0][0]\n",
    "        y_list.append(list_classes[idx])\n",
    "    return y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "democratic-latvia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     age  sex  cp  trtbps  chol  fbs  restecg  thalachh  exng  oldpeak  slp  \\\n",
      "0     63    1   3     145   233    1        0       150     0      2.3    0   \n",
      "1     37    1   2     130   250    0        1       187     0      3.5    0   \n",
      "2     41    0   1     130   204    0        0       172     0      1.4    2   \n",
      "3     56    1   1     120   236    0        1       178     0      0.8    2   \n",
      "4     57    0   0     120   354    0        1       163     1      0.6    2   \n",
      "..   ...  ...  ..     ...   ...  ...      ...       ...   ...      ...  ...   \n",
      "298   57    0   0     140   241    0        1       123     1      0.2    1   \n",
      "299   45    1   3     110   264    0        1       132     0      1.2    1   \n",
      "300   68    1   0     144   193    1        1       141     0      3.4    1   \n",
      "301   57    1   0     130   131    0        1       115     1      1.2    1   \n",
      "302   57    0   1     130   236    0        0       174     0      0.0    1   \n",
      "\n",
      "     caa  thall  output  \n",
      "0      0      1       1  \n",
      "1      0      2       1  \n",
      "2      0      2       1  \n",
      "3      0      2       1  \n",
      "4      0      2       1  \n",
      "..   ...    ...     ...  \n",
      "298    0      3       0  \n",
      "299    0      3       0  \n",
      "300    2      3       0  \n",
      "301    1      3       0  \n",
      "302    1      2       0  \n",
      "\n",
      "[303 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "# read csv data\n",
    "heart = pd.read_csv(\"heart.csv\")\n",
    "print(heart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "frequent-sector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 13)\n",
      "(303,)\n"
     ]
    }
   ],
   "source": [
    "# separate X and y\n",
    "X = np.array(heart.iloc[:,:-1])\n",
    "print(X.shape)\n",
    "y = np.array(heart.iloc[:,13])\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "tropical-basket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 27)\n"
     ]
    }
   ],
   "source": [
    "# prevent overflow warning\n",
    "X[:,np.r_[0,3,4,7]] = X[:,np.r_[0,3,4,7]]/100\n",
    "\n",
    "# onehot2list categorical variables\n",
    "X = np.delete(X, [2,10,11,12],1)\n",
    "cp = list2onehot(heart.iloc[:,2].tolist(), [0,1,2,3]) \n",
    "slp = list2onehot(heart.iloc[:,10].tolist(), [0,1,2]) #what is this\n",
    "caa = list2onehot(heart.iloc[:,11].tolist(), [0,1,2,3]) #should this be one hot?\n",
    "thall = list2onehot(heart.iloc[:,12].tolist(), [0,1,2,3]) #what is this\n",
    "restecg = list2onehot(heart.iloc[:,6].tolist(), [0,1,2]) #what is this\n",
    "\n",
    "X = np.hstack((X, cp, slp, thall, restecg, caa))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "binary-village",
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = list2onehot(y.tolist(), [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "recognized-sullivan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (242, 27)\n",
      "X_test.shape (61, 27)\n",
      "y_train.shape (242, 1)\n",
      "y_test.shape (61, 1)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into train and test sets\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "y_train = []\n",
    "\n",
    "for i in np.arange(X.shape[0]):\n",
    "    # for each example i, make it into train set with probabiliy 0.8 and into test set otherwise \n",
    "    U = np.random.rand() # Uniform([0,1]) variable\n",
    "    if U<0.8:\n",
    "        X_train.append(X[i,:])\n",
    "        y_train.append(y[i])\n",
    "    else:\n",
    "        X_test.append(X[i,:])\n",
    "        y_test.append(y[i])\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "X_test = np.asarray(X_test)\n",
    "y_train = np.asarray(y_train).reshape(-1,1)\n",
    "y_test = np.asarray(y_test).reshape(-1,1)\n",
    "\n",
    "print('X_train.shape', X_train.shape)\n",
    "print('X_test.shape', X_test.shape)\n",
    "print('y_train.shape', y_train.shape)\n",
    "print('y_test.shape', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6bfa012e-4ade-4547-b9ed-574026cc1872",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class DeepFFNN(object):\n",
    "    \"\"\"\n",
    "    Author: Hanbaek Lyu\n",
    "    Genearal Deep Feedforward Neural Network implementation \n",
    "    Input data type: training_data = [pattern1, pattern2, ..., pattern n]\n",
    "    Activation: tanh for hidden layer and sigmoid for output layer \n",
    "    \n",
    "    pattern i = [np.array (input), np.array (output)]\n",
    "    \n",
    "    TODO: Currently uses square loss. Should be easy to implement other loss functions. \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 hidden_layer_sizes,  # input and output layer sizes read off from training data\n",
    "                 training_data,  # list of patterns [np.array (input), np.array (output)]\n",
    "                 activation_list=None): # desired list of activation functions in each layer. \n",
    "       \n",
    "        # initialize training data and layer info\n",
    "        self.training_data = training_data\n",
    "        self.activation_list = activation_list\n",
    "        self.list_layer_sizes = [len(self.training_data[0][0]) + 1] + hidden_layer_sizes + [len(self.training_data[0][1])]        \n",
    "        # add hidden unit in the input layer. No hidden units for the hidden layers. \n",
    "        self.n_layers = len(self.list_layer_sizes)-1\n",
    "        \n",
    "        self.initialize()\n",
    "        \n",
    "    def initialize(self):\n",
    "        \n",
    "        # list of activation functions\n",
    "        if self.activation_list is None:\n",
    "            activation_list = ['tanh' for i in np.arange(len(self.list_layer_sizes))]\n",
    "            activation_list[0] = 'identity'  # dummy activation for the input layer\n",
    "            activation_list[-1] = 'sigmoid'\n",
    "            self.activation_list = activation_list\n",
    "        # default activation of nodes\n",
    "        node_states = []\n",
    "        for i in np.arange(len(self.list_layer_sizes)):\n",
    "            node_states.append(np.zeros(shape=[self.list_layer_sizes[i], ]))\n",
    "        self.node_states = node_states\n",
    "        \n",
    "        # initial weight matrices \n",
    "        # use scheme from 'efficient backprop to initialize weights'\n",
    "        weight_matrices = []\n",
    "        for i in np.arange(self.n_layers):\n",
    "            weight_range = 1/(self.list_layer_sizes[i]**(0.5))\n",
    "            U = np.random.normal(loc = 0, scale = weight_range, size = (self.list_layer_sizes[i], self.list_layer_sizes[i+1]))\n",
    "            weight_matrices.append(U)\n",
    "        self.weight_matrices = weight_matrices\n",
    "           \n",
    "        # create arrays of 0's to store previous gradients for momentum term in SGD update \n",
    "        prev_grad_list = []\n",
    "        for i in np.arange(self.n_layers):\n",
    "            V = np.zeros((self.list_layer_sizes[i], self.list_layer_sizes[i+1]))\n",
    "            prev_grad_list.append(V)\n",
    "        self.prev_grad_list = prev_grad_list\n",
    "\n",
    "    def forwardPropagate(self, inputs):\n",
    "        # Forward propagate the input using the current weights and update node states \n",
    "        self.node_states[0][:-1] = inputs # avoid last coordinate for hidden unit \n",
    "        for i in np.arange(self.n_layers):    \n",
    "            X_new = self.node_states[i].T @ self.weight_matrices[i]\n",
    "            X_new = activation(X_new, type=self.activation_list[i+1])\n",
    "            self.node_states[i+1] = X_new\n",
    "        \n",
    "        return self.node_states[-1]\n",
    "\n",
    "    def backPropagate(self, targets):\n",
    "        \"\"\"\n",
    "        Backpropagate errors from the output to the input layer \n",
    "        Return gradients for the weight matrices\n",
    "        \"\"\"\n",
    "    \n",
    "        error_list = self.node_states.copy()\n",
    "        # error at the output layer to be backpropagated \n",
    "        error = -(np.asarray(targets) - np.asarray(self.node_states[-1]))\n",
    "        for L in range(self.n_layers, 0, -1): # layer index to be backpropagated \n",
    "            # print('L', L)\n",
    "            if L < self.n_layers: # Not backpropagating from the output layer\n",
    "                error = self.weight_matrices[L] @ error_list[L+1].reshape(-1,1)\n",
    "                error = error[:,0] \n",
    "            error_list[L] = delta_activation(self.node_states[L], type=self.activation_list[L]) * error\n",
    "            \n",
    "        # Compute the gradients\n",
    "        grad_list = self.weight_matrices.copy()\n",
    "        for i in np.arange(self.n_layers):\n",
    "            grad_list[i] = self.node_states[i].reshape(-1,1) @ error_list[i+1].reshape(1,-1) \n",
    "        \n",
    "        return grad_list\n",
    "\n",
    "\n",
    "    def train(self, iterations=100, learning_rate=0.5, momentum=0.5, rate_decay=0.01, verbose=True):\n",
    "        # N: learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.rate_decay = rate_decay\n",
    "        error = 10\n",
    "        i=0\n",
    "        while (i<iterations) and (error>0.001):\n",
    "            error = 0.0\n",
    "            random.shuffle(self.training_data)\n",
    "            for p in self.training_data:\n",
    "                inputs = p[0]\n",
    "                targets = p[1]\n",
    "                self.forwardPropagate(inputs)\n",
    "                grad_list = self.backPropagate(targets)\n",
    "                \n",
    "                for L in np.arange(self.n_layers):\n",
    "                    # update the L th weight matrix connecting L th and (L+1)st layers \n",
    "                    grad = grad_list[L]\n",
    "                    prev_grad = self.prev_grad_list[L]\n",
    "                    self.weight_matrices[L] -= self.learning_rate * grad + self.momentum * prev_grad\n",
    "                    self.prev_grad_list[L] = grad # store current gradient \n",
    "        \n",
    "                error += (0.5) * np.linalg.norm(np.asarray(targets) - self.node_states[-1])**2\n",
    "            \n",
    "            with open('error.txt', 'a') as errorfile:\n",
    "                errorfile.write(str(error) + '\\n')\n",
    "                errorfile.close()\n",
    "                \n",
    "            if (i % 5 == 0) and verbose:\n",
    "                print('iteration %i, error %-.5f' % (i, error))\n",
    "            # learning rate decay\n",
    "            self.learning_rate = 1/(np.log(i+2) * (i+50)**(0.5))\n",
    "            # self.learning_rate = self.learning_rate * (self.learning_rate / (self.learning_rate + (self.learning_rate * self.rate_decay)))\n",
    "            \n",
    "            i += 1  \n",
    "        \n",
    "    \n",
    "    def predict(self, X, normalize = False):\n",
    "        X = np.asarray(X).T\n",
    "        x = np.vstack((np.asarray(X), np.ones(X.shape[1]))) # add 1 for hidden units in the input layer\n",
    "        print('X.shape', X.shape)\n",
    "    \n",
    "        for i in np.arange(self.n_layers):    \n",
    "            x = x.T @ self.weight_matrices[i]\n",
    "            x = activation(x.T, type=self.activation_list[i+1])\n",
    "            \n",
    "        print('y_hat.shape', x.shape)\n",
    "        return x\n",
    "    \n",
    "### Helper functions     \n",
    "    \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# derivative of sigmoid\n",
    "def dsigmoid(y):\n",
    "    return y * (1.0 - y)\n",
    "\n",
    "# using tanh over logistic sigmoid is recommended   \n",
    "\n",
    "def tanh(x):\n",
    "    return (1-np.exp(-2*x))/(1+np.exp(-2*x))\n",
    "    # return np.tanh(x)\n",
    "    \n",
    "# derivative for tanh sigmoid\n",
    "def dtanh(y):\n",
    "    return 1 - y*y\n",
    "\n",
    "\n",
    "### Helper functions\n",
    "\n",
    "def loss_function(y, y_hat, type='cross-entropy'):\n",
    "    \"\"\"\n",
    "    y_hat = column array of predictive PMF \n",
    "    y = column array of one-hot encoding of true class label\n",
    "    \"\"\"\n",
    "    if type == 'cross_entropy':\n",
    "        return cross_entropy(y=y, y_hat=y_hat)\n",
    "    elif type == 'square':\n",
    "        return (1/2) * (y_hat - y).T @ (y_hat - y)\n",
    "    elif type == 'softmax-cross-entropy':\n",
    "        return cross_entropy(y=y, y_hat=softmax(y_hat))\n",
    "   \n",
    "\n",
    "def delta_loss_function(y, y_hat, type='cross-entropy'):\n",
    "    \"\"\"\n",
    "    y_hat = column array of predictive PMF \n",
    "    y = column array of one-hot encoding of true class label\n",
    "    \"\"\"\n",
    "    # return delta_cross_entropy(y=y, y_hat=y_hat/np.sum(y_hat))\n",
    "    \n",
    "    if type == 'cross-entropy':\n",
    "        return delta_cross_entropy(y=y, y_hat=y_hat)\n",
    "    elif type == 'square':\n",
    "        return y_hat - y\n",
    "    elif type == 'softmax-cross-entropy':\n",
    "        return softmax(y_hat) - y\n",
    "\n",
    "        \n",
    "def activation(x, type='sigmoid'):\n",
    "    if type == 'sigmoid':\n",
    "        return 1/(1+np.exp(-x))\n",
    "    elif type == 'ReLU':\n",
    "        return np.maximum(0,x)\n",
    "    elif type == 'tanh':\n",
    "        return tanh(x)\n",
    "    elif type == 'identity':\n",
    "        return x\n",
    "    \n",
    "def delta_activation(y, type='sigmoid'):\n",
    "    # derivate of activation function\n",
    "    if type == 'sigmoid':\n",
    "        return y*(1-y)\n",
    "    elif type == 'ReLU':\n",
    "        return int((y>0))\n",
    "    elif type == 'tanh':\n",
    "        return 1-y**2\n",
    "    elif type == 'identity':\n",
    "        return 1\n",
    "        \n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x))\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def cross_entropy(y, y_hat):\n",
    "    \"\"\"\n",
    "    y_hat = column array of predictive PMF \n",
    "    y = column array of one-hot encoding of true class label\n",
    "    \"\"\"\n",
    "    return -(y.T @ np.log(y_hat))[0][0]\n",
    "\n",
    "def delta_cross_entropy(y, y_hat):\n",
    "    \"\"\"\n",
    "    y_hat = column array of predictive PMF \n",
    "    y = column array of one-hot encoding of true class label\n",
    "    \"\"\"\n",
    "    y_hat /= np.max(y_hat)\n",
    "    z = y.copy()\n",
    "    for i in np.arange(y.shape[0]):\n",
    "        a = y.argmax(axis=0)[0]\n",
    "        z[i,0] = -1/y_hat[a, 0]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "68f2a58b-e7b2-4a01-9c7b-4f55f776eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_metrics(Y_test, P_pred, use_opt_threshold=False, verbose=True):\n",
    "    \n",
    "    # y_test = binary label \n",
    "    # P_pred = predicted probability for y_test\n",
    "    # compuate various binary classification accuracy metrics\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(Y_test, P_pred, pos_label=None)\n",
    "    mythre = thresholds[np.argmax(tpr - fpr)]\n",
    "    myauc = metrics.auc(fpr, tpr)\n",
    "    # print('!!! auc', myauc)\n",
    "    \n",
    "    # Compute classification statistics\n",
    "    threshold = 0.5\n",
    "    if use_opt_threshold:\n",
    "        threshold = mythre\n",
    "    \n",
    "    Y_pred = P_pred.copy()\n",
    "    Y_pred[Y_pred < threshold] = 0\n",
    "    Y_pred[Y_pred >= threshold] = 1\n",
    "\n",
    "    mcm = confusion_matrix(Y_test, Y_pred)\n",
    "    \n",
    "    tn = mcm[0, 0]\n",
    "    tp = mcm[1, 1]\n",
    "    fn = mcm[1, 0]\n",
    "    fp = mcm[0, 1]\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    sensitivity = tn / (tn + fp)\n",
    "    specificity = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    fall_out = fp / (fp + tn)\n",
    "    miss_rate = fn / (fn + tp)\n",
    "\n",
    "    # Save results\n",
    "    results_dict = {}\n",
    "    results_dict.update({'Y_test': Y_test})\n",
    "    results_dict.update({'Y_pred': Y_pred})\n",
    "    results_dict.update({'AUC': myauc})\n",
    "    results_dict.update({'Opt_threshold': mythre})\n",
    "    results_dict.update({'Accuracy': accuracy})\n",
    "    results_dict.update({'Sensitivity': sensitivity})\n",
    "    results_dict.update({'Specificity': specificity})\n",
    "    results_dict.update({'Precision': precision})\n",
    "    results_dict.update({'Fall_out': fall_out})\n",
    "    results_dict.update({'Miss_rate': miss_rate})\n",
    "    results_dict.update({'Confusion_mx': mcm})\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        for key in [key for key in results_dict.keys()]:\n",
    "            if key not in ['Y_test', 'Y_pred', 'Confusion_mx']:\n",
    "                print('% s ===> %.3f' % (key, results_dict.get(key)))\n",
    "        print('Confusion matrix \\n ===>', mcm)\n",
    "            \n",
    "    return results_dict, Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce1c711b-4976-4797-b896-818b75e317e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confusion_matrix(y, yhat, classes):\n",
    "    '''\n",
    "        Draws a confusion matrix for the given target and predictions\n",
    "        Adapted from scikit-learn and discussion example.\n",
    "    '''\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    matrix = confusion_matrix(y, yhat)\n",
    "    plt.imshow(matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    num_classes = len(classes)\n",
    "    plt.xticks(np.arange(num_classes), classes, rotation=90)\n",
    "    plt.yticks(np.arange(num_classes), classes)\n",
    "    \n",
    "    fmt = 'd'\n",
    "    thresh = matrix.max() / 2.\n",
    "    for i, j in itertools.product(range(matrix.shape[0]), range(matrix.shape[1])):\n",
    "        plt.text(j, i, format(matrix[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f8f0d6b-ab66-4fe5-91b6-a99d1ed8d9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, error 43.19119\n",
      "iteration 5, error 24.04512\n",
      "iteration 10, error 23.05870\n",
      "iteration 15, error 22.10144\n",
      "iteration 20, error 21.69590\n",
      "iteration 25, error 21.16107\n",
      "iteration 30, error 20.74605\n",
      "iteration 35, error 19.98387\n",
      "iteration 40, error 20.12112\n",
      "iteration 45, error 19.81058\n",
      "iteration 50, error 19.20362\n",
      "iteration 55, error 19.05660\n",
      "iteration 60, error 18.84456\n",
      "iteration 65, error 18.42071\n",
      "iteration 70, error 18.20455\n",
      "iteration 75, error 17.85300\n",
      "iteration 80, error 17.60644\n",
      "iteration 85, error 17.49218\n",
      "iteration 90, error 17.09805\n",
      "iteration 95, error 16.67092\n",
      "X.shape (27, 61)\n",
      "y_hat.shape (2, 61)\n",
      "Confusion Matrix: \n",
      "\n",
      "AUC ===> 0.927\n",
      "Opt_threshold ===> 0.681\n",
      "Accuracy ===> 0.869\n",
      "Sensitivity ===> 0.731\n",
      "Specificity ===> 0.971\n",
      "Precision ===> 0.829\n",
      "Fall_out ===> 0.269\n",
      "Miss_rate ===> 0.029\n",
      "Confusion matrix \n",
      " ===> [[19  7]\n",
      " [ 1 34]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y0 = list2onehot(y_train, [0,1])\n",
    "\n",
    "out = []\n",
    "# populate the tuple list with the data\n",
    "for i in range(X_train.shape[0]):\n",
    "    item = list((X_train[i,:], y0[i,:])) \n",
    "    out.append(item)\n",
    "    \n",
    "    \n",
    "\n",
    "# FFNN training\n",
    "NN = DeepFFNN(hidden_layer_sizes=[5], training_data = out)\n",
    "NN.train(iterations=100, learning_rate = 0.5, momentum = 0, rate_decay = 0.01)\n",
    "\n",
    "\n",
    "# FFNN prediction\n",
    "\n",
    "X_test /= np.max(X_test)\n",
    "out_test = []\n",
    "for i in range(X_test.shape[0]):\n",
    "    out_test.append(X_test[i,:].tolist())\n",
    "\n",
    "y_hat = NN.predict(out_test).T\n",
    "\n",
    "y_test_label = np.asarray(y_test)\n",
    "\n",
    "P_pred = np.asarray([p[1] for p in y_hat])\n",
    "\n",
    "\n",
    "print(\"Confusion Matrix: \\n\")\n",
    "metrics, y_preedd = compute_accuracy_metrics(Y_test=y_test, P_pred=P_pred, use_opt_threshold=False, verbose=True)\n",
    "draw_confusion_matrix(y_test, y_preedd, ['not sick', 'sick'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
